{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9161096d-a769-4995-acc2-1b7611dcd5b3",
   "metadata": {},
   "source": [
    "# Week 5, Data science in Neuroscience\n",
    "\n",
    "\n",
    "## Plan for this week\n",
    "\n",
    "1. Introduction to pytorch\n",
    "2. Tensors\n",
    "3. Training loop in pytorch\n",
    "4. A first neural network\n",
    "5. Convolutional networks to process images\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dcbbc1-2c6a-409d-9f0b-47f8f736197a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convolutional neural networks (CNN)\n",
    "\n",
    "Date back to 1989 (Yann LeCun). \n",
    "\n",
    "Convolutional neural network revolutionized how images are processed by neural network. \n",
    "\n",
    "They are used for all sorts of problems involving image processing.\n",
    "\n",
    "The artificial neurons in the network have receptive fields, similar to what is observed in the visual system.\n",
    "\n",
    "Why they work:\n",
    "\n",
    "* They keep information about the spatial arrangement of the input image.\n",
    "* Mechanism to detect a set a feature, independently of their position in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a26923-0db3-4506-83c4-227b23354c32",
   "metadata": {},
   "source": [
    "## Convolution \n",
    "\n",
    "<div>\n",
    "<img src=\"../images/convolution1.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "## LeNet: one of the first convolutional neural network.\n",
    "\n",
    "LeCun et al. (1989). Backpropagation applied to handwritten zip code recognition. Neural Computation.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/LeNet5.png\" width=\"1200\"/>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f889a-1a19-47ce-a92c-230ba0ecbe84",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks as feature detectors\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/imageNet_features_01.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/imageNet_features_02.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/imageNet_features_03.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/imageNet_features_04.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27170ff-f047-4931-97d1-d7cc4b970941",
   "metadata": {},
   "source": [
    "## ImageNet competition: 1.3 million images and 1000 classes\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/imageNetResults.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c733d6-b5dd-4a4c-8c0c-23f7df6b08bc",
   "metadata": {},
   "source": [
    "# Let's build and train a convolutional neural network\n",
    "\n",
    "Our aim will be to build a convolutional neural network to identify what object is in images. This is a classification problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4adfdb6-d811-4420-a98b-5819224e9d33",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We need a dataset of imagest to train our model.\n",
    "\n",
    "We download our dataset using torchvision.datasets. We will use the CIFAR-10 dataset. It consists of 60000 small 32x32 color images.\n",
    "\n",
    "https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0d65d4-ddfe-41d8-8503-d5cc776a401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5446d36b-b326-4b90-a544-007c7cd80fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817ac67e-9779-49f8-91c1-a050699344bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ../data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ae3378-3e8d-404a-8ae1-7316815c2c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ../data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1ecb15-3780-4c85-8fb3-f2cbd9cb7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label = train_dataset[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9271ac9-661d-4d33-a56d-f20dec98cb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c39408-3d5e-416d-8160-c94021aacd87",
   "metadata": {},
   "source": [
    "The shame is [Color, Height, Width]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8cf66e-ac35-4635-b549-d3e600a26218",
   "metadata": {},
   "source": [
    "The first dimension of size 3 represent the color channels. In this case, we only have one color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1476b4-cf7a-4043-a910-8459e0caaf79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e247f-b549-49ae-bb82-b583b20af699",
   "metadata": {},
   "source": [
    "The labels are just numbers. We can associate a word to these numbers by creating a list of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869120bf-7e4e-44e9-abf5-ac3de949e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1e4317-7b86-4edf-b10a-66d8f98f34b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011804a-5deb-4678-aa6d-23deafcc1607",
   "metadata": {},
   "source": [
    "Plot a few images and labels from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8dd1b70-5b7e-4741-9d9c-a0732368b0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b224e7-27e8-47f0-8e7c-d085779c2923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.permute(1,2,0)) # we use permute because imshow() wants [Height,Width,Color] and the tensor is [Color,Height,Width].\n",
    "plt.title(\"This is a {}\".format(class_names[label]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd60794c-ec36-4a68-ab97-f9525fbbcc25",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a figure with several subplots. In each subplot, show the image with imshow() and put the label as title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1602a3-bec8-44d9-b0c9-553d830d03e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4304e70-7e12-4eb1-b9b2-6e3d6cbd8e6c",
   "metadata": {},
   "source": [
    "## Transforms and normalization\n",
    "\n",
    "To facilitate learning:\n",
    "\n",
    "* Normalize our input data so that they have a mean of 0 and a standard deviation of 1\n",
    "* Make sure that all features or color channels have the same distribution. \n",
    "\n",
    "Making sure that the different features have the same distribution means that the choosen learning rate will be appropriate for all features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85665593-8f4e-43f8-95af-ba7dcee179e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = torch.stack([img for img,_ in train_dataset],dim=3)\n",
    "#a.shape\n",
    "#a.view(3,-1).mean(1),a.view(3,-1).std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8867fff-da07-4bf2-b750-e232dba81548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                           torchvision.transforms.Normalize((0.4915,0.4823,0.4468), # means of each color channel\n",
    "                                                                            (0.2470,0.2435,0.2616))]) # standard deviation of each color channel\n",
    "\n",
    "# We pass the transform function to the dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cec511e-0bba-4e1e-a4bb-9f11a40c98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = torch.stack([img for img,_ in train_dataset],dim=3)\n",
    "#a.shape\n",
    "#a.view(3,-1).mean(1),a.view(3,-1).std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f7eda2e-289e-4d75-a5b1-e20895f20c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label = train_dataset[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce1fcc8b-ea16-49ca-947a-135e85637fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.permute(1,2,0)) # we use permute because imshow() wants H x W x C and the tensor is C X H X W.\n",
    "plt.title(\"This is a {}\".format(class_names[label]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886d1b4-51f1-449c-985f-38ddcb3c86e1",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "We usually want to feed a few images at a time to our neural network. With large models and datasets, we would not be able to fit all that data in the GPU or computer memory. So we feed a few items at a time. \n",
    "\n",
    "The small group of images that we are feeding to the network simultaneously is called a `batch`.\n",
    "\n",
    "The number of images that we feed to the neural network simultaneously is called the `batch size`.\n",
    "\n",
    "pytorch has a `dataloader` class that can help you getting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2ac7cca-4a95-4a5a-813d-9d38556d1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True, \n",
    "                                           num_workers=2, \n",
    "                                           pin_memory=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef8ff6a0-c7c7-4e24-973a-6cf83b68ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96aedc2c-ecf3-4519-a3e4-219cf6188f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac626ea-b370-48f4-bd28-9ac7342d39d7",
   "metadata": {},
   "source": [
    "The shape is [Batch, Color, Height, Width]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5199294-033c-452f-a8ba-9ad983ad40f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941daf62-af7b-41cd-9846-54a613b4fbd3",
   "metadata": {},
   "source": [
    "## Our model\n",
    "\n",
    "We can build our model using the class from the last class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6f32f-6dfc-48ea-a496-8877033a51c0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../images/LeNet5.png\" width=\"1200\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92a941bc-b409-4be1-9410-f5d81d862820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Convolutional_model(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to create convolutional neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializer. Runs when an object is created\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # input images will be batch_size,3,32,32\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels= 6, kernel_size=5,stride=1, padding=0)  # output shape: [batch_size, 6, 28, 28]\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2) # a max poll operation, # output shape: [batch_size, 6, 14,14]\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5,stride=1, padding=0)  # output shape: [batch_size, 16, 10, 10]\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)                                                   # output shape: [batch_size, 16 , 5, 5]\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 16 * 5 * 5, out_features=120) # here we need to know the dimension of the data coming in (following second pool operation)\n",
    "        self.fc2 = nn.Linear(in_features = 120, out_features =84)\n",
    "        self.fc3 = nn.Linear(in_features = 84, out_features=10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Make predictions with our model\n",
    "        \"\"\"\n",
    "\n",
    "        # We have 2 x pool(relu(conv()))\n",
    "        # This is the part extracting visual features\n",
    "        \n",
    "        # input shape [batch,3,32,32]\n",
    "        x = F.relu(self.conv1(x)) # [batch, 6, 28, 28]\n",
    "        x = self.pool1(x) # [batch, 6, 14, 14]\n",
    "        \n",
    "        x = F.relu(self.conv2(x)) # [batch, 16, 10, 10]\n",
    "        x = self.pool2(x) # [batch, 16, 5, 5]\n",
    "\n",
    "        # We then have 3 linear layers. These layers are take visual features as inputs and find a way to classify the image.\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch, needed for the linear layer, [batch, 400]\n",
    "        x = F.relu(self.fc1(x)) # [batch, 120]\n",
    "        x = F.relu(self.fc2(x)) # [batch, 84]\n",
    "        x = self.fc3(x) # [batch, 10]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f987f82-9893-4b14-bf0a-2c563514aed6",
   "metadata": {},
   "source": [
    "**If you are implementing a model, start by passing the data through it line by line**\n",
    "\n",
    "To follow how each layer in your network modify the dimension of the data flowing through the network, you can run it line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b09a1e8-e27f-4442-aeb5-a626407e9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=3,out_channels=6,kernel_size=5,stride=1,padding=0)\n",
    "pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "conv2 = nn.Conv2d(in_channels=6,out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "pool2 = nn.MaxPool2d(kernel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "812c78ec-c0ab-436d-8f50-3455f03d4b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([4, 3, 32, 32])\n",
      "output of conv1 shape: torch.Size([4, 6, 28, 28])\n",
      "output of pool shape: torch.Size([4, 6, 14, 14])\n",
      "output of conv2 shape: torch.Size([4, 16, 10, 10])\n",
      "output of pool shape: torch.Size([4, 16, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"images.shape:\",images.shape)\n",
    "res = F.relu(conv1(images))\n",
    "print(\"output of conv1 shape:\", res.shape)\n",
    "res = pool1(res)\n",
    "print(\"output of pool shape:\", res.shape)\n",
    "res = F.relu(conv2(res))\n",
    "print(\"output of conv2 shape:\", res.shape)\n",
    "res = pool2(res)\n",
    "print(\"output of pool shape:\", res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7291ef0e-070b-473e-98bf-e61e5058e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Convolutional_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb93852c-08b6-424e-9b26-7e8b5506de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name,p in conv_model.named_parameters():\n",
    "#    print(name,p.shape,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c008e7c-0778-47c7-a14d-b69ea7eee7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 62006\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters:\", sum(p.numel() for p in conv_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815230e-1da6-414a-a06e-ec4a9c5b8a55",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "We need a loss function that works with categorical data. \n",
    "\n",
    "The last layer has an output size of 10, which is the number of categories we have in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1912a980-5b7b-4f2b-9f94-3a14781433de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(conv_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65aa380-1faf-46ad-a100-2c2ce8712c1b",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b2c4ab6-3311-4f9c-8bcc-bc401778cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, loss_fn, model, train_dataloader, test_dataloader):\n",
    "    for epoch in range (n_epochs):\n",
    "        \n",
    "        loss_sum = 0.0\n",
    "        for batch_no, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            data, labels = data\n",
    "        \n",
    "        \n",
    "            yhat = model(data) # make predictions\n",
    "            loss = loss_fn(yhat,labels) # calculate the loss\n",
    "            \n",
    "            optimizer.zero_grad() # zero the gradients of our model's parameters\n",
    "            loss.backward() # calculate gradients of the model's parameters\n",
    "            optimizer.step() # will change the model parameters to reduce the loss\n",
    "        \n",
    "            loss_sum = loss_sum + loss.item()\n",
    "            \n",
    "            if batch_no % 2000 == 1999:\n",
    "                print(\"Epoch: {}, batch: {} Loss: {}\".format(epoch,batch_no, loss_sum/2000))\n",
    "                loss_sum=0.0\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1c5400c-b551-40fa-8ada-06232f9f3c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch: 1999 Loss: 2.1502312347888948\n",
      "Epoch: 0, batch: 3999 Loss: 1.752985846042633\n",
      "Epoch: 0, batch: 5999 Loss: 1.6250043596923351\n",
      "Epoch: 0, batch: 7999 Loss: 1.5480455251783132\n",
      "Epoch: 0, batch: 9999 Loss: 1.478548425734043\n",
      "Epoch: 0, batch: 11999 Loss: 1.455347571849823\n",
      "Epoch: 1, batch: 1999 Loss: 1.3632015668451787\n",
      "Epoch: 1, batch: 3999 Loss: 1.3444422507211566\n",
      "Epoch: 1, batch: 5999 Loss: 1.3458413817547261\n",
      "Epoch: 1, batch: 7999 Loss: 1.2990014576613904\n",
      "Epoch: 1, batch: 9999 Loss: 1.3051448163315653\n",
      "Epoch: 1, batch: 11999 Loss: 1.2639150146245957\n",
      "Epoch: 2, batch: 1999 Loss: 1.2026023428365589\n",
      "Epoch: 2, batch: 3999 Loss: 1.2076302549429239\n",
      "Epoch: 2, batch: 5999 Loss: 1.2022928779181092\n",
      "Epoch: 2, batch: 7999 Loss: 1.1903982744878159\n",
      "Epoch: 2, batch: 9999 Loss: 1.1983077215515077\n",
      "Epoch: 2, batch: 11999 Loss: 1.176746095146984\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs=3,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=loss_fn,\n",
    "              model=conv_model,\n",
    "             train_dataloader=train_dataloader,\n",
    "             test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e444fdaa-717c-48f2-8fa6-fc8ffc4cd269",
   "metadata": {},
   "source": [
    "## Saving your model to file \n",
    "\n",
    "With larger models trained on large datasets, you will want to save your trained model so that you don't have to train it from scratch all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "485d8d3c-e8d4-494d-b339-dcc772432a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../models/cifar10_conv_model.pth'\n",
    "torch.save(conv_model.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a39a0-6247-4b73-866e-2fc47cbf3edb",
   "metadata": {},
   "source": [
    "## Loading your model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0467cb58-78d6-4c00-a7ae-b33cc57d2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../models/cifar10_conv_model.pth'\n",
    "loaded_state_dict = torch.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9841c547-f88c-40a3-b2e6-502b08417d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = Convolutional_model()\n",
    "conv_model.load_state_dict(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170539d3-2b77-4f6a-bb4e-47af0965110d",
   "metadata": {},
   "source": [
    "## Evaluate our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3c93866-2c70-4d15-87f5-0894de2f0c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81462fc0-8a97-4092-b250-762cf9041310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 8, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(dataiter)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a01ccbb-b0e8-43b8-a6e3-9f6b0e995cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    res = conv_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42a2b128-3f73-49bd-bbbe-2ab863f88733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9617, -0.5399, -0.7857,  1.4555, -0.2357,  1.7323, -1.0513,  0.7570,\n",
       "         -0.4340,  0.5671],\n",
       "        [ 5.3028,  9.5175, -3.3490, -4.0638, -3.5460, -5.6737, -2.5052, -6.0921,\n",
       "          5.3037,  5.5765],\n",
       "        [ 1.9319,  2.7439, -0.5220, -1.1251, -0.0719, -2.2358, -1.3244, -1.9506,\n",
       "          2.0647,  1.4760],\n",
       "        [ 1.9239,  2.3232, -1.0252, -1.4727, -0.9704, -2.2731, -2.0417, -1.4276,\n",
       "          2.6754,  1.5120]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c6ea4-5256-4cdc-a0a9-65cdda2165f1",
   "metadata": {},
   "source": [
    "We can use the `torch.max()` function to get the index with the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "447ccd94-676b-42b3-bc2c-88d02924dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions = torch.max(res,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23901b84-371e-4ba9-b117-7431f987ec47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 1, 1, 8])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c2a8693-04e3-4edc-8e73-78873beb3a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(labels == predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b06dfe5e-10a4-4814-89cb-f526777150a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_accuracy(model, dataloader):\n",
    "    \"\"\"\n",
    "    Function to calculate the classification accuracy of a model on a given dataset.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return correct*100/total, correct,total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1eb7c630-dc13-4d6d-9f56-e352b0148c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy,_,_ = evaluate_model_accuracy(conv_model,train_dataloader)\n",
    "test_accuracy,_,_  = evaluate_model_accuracy(conv_model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c9d8d69-eff9-4f54-9802-acd37c9e7b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 61.626\n",
      "Accuracy on test set: 57.59\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training set:\", train_accuracy)\n",
    "print(\"Accuracy on test set:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d58b5034-6b18-4955-a8c4-ae426e635426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: plane is 48.9 %\n",
      "Accuracy for class: car   is 78.7 %\n",
      "Accuracy for class: bird  is 44.0 %\n",
      "Accuracy for class: cat   is 24.3 %\n",
      "Accuracy for class: deer  is 61.5 %\n",
      "Accuracy for class: dog   is 43.4 %\n",
      "Accuracy for class: frog  is 75.0 %\n",
      "Accuracy for class: horse is 66.8 %\n",
      "Accuracy for class: ship  is 63.0 %\n",
      "Accuracy for class: truck is 70.3 %\n"
     ]
    }
   ],
   "source": [
    "correct_pred = {classname: 0 for classname in class_names}\n",
    "total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = conv_model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[class_names[label]] += 1\n",
    "            total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d96b58d-388c-4f31-a815-ca226401d294",
   "metadata": {},
   "source": [
    "You now know how Google and co. classify the images that you upload to their servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6adeb-e96e-4d30-bd54-bc75efd0098a",
   "metadata": {},
   "source": [
    "## Let's have a look at the filter in our first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b900b6a2-0418-46ee-9b18-4149ec579393",
   "metadata": {},
   "outputs": [],
   "source": [
    "myIter = iter(conv_model.named_parameters())\n",
    "name, param = next(myIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84e1cc19-56e5-4ca9-90a5-067e1a00a7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('conv1.weight', torch.Size([6, 3, 5, 5]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "829a5a96-5cdc-41fd-a4ca-0449f82afe84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7277172803878784, 0.646938145160675)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmin = param.min().detach().numpy().item()\n",
    "pmax = param.max().detach().numpy().item()\n",
    "pmin,pmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bed7f1dc-146c-41af-8881-4107d2a27abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x300 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow=1\n",
    "ncol=param.shape[0]\n",
    "fig, ax = plt.subplots(nrow,ncol,figsize=(15,3))\n",
    "for i in range(param.shape[0]):\n",
    "    f = filter0 = param[i].permute(1,2,0).detach().numpy()\n",
    "    f = (f-pmin)/(pmax-pmin)\n",
    "    ax[i].imshow(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988cbe3-79a1-434c-81e4-2061c9c15ad5",
   "metadata": {},
   "source": [
    "## Training networks using a GPU\n",
    "\n",
    "In this notebook, we trained a very small convolutional neural network on a set of very small images. You can train these models with the CPU and it is almost as fast as using a GPU.\n",
    "\n",
    "In most applications, you would be working with a larger convolutional network (e.g., [ResNet](https://pytorch.org/vision/main/models/resnet.html)) with images that are often more than 28x28 pixels (e.g., [ImageNet](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageNet.html)).\n",
    "\n",
    "For such task, training is often more than 10x faster using a GPU instead of a CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c918cc6-8fbd-47cf-9f1c-0d3d6f4599a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c54fbd-6d6e-4b63-b40f-bbb0c3b6df1a",
   "metadata": {},
   "source": [
    "You need to make two changes to run your model on a GPU (if you have one)\n",
    "\n",
    "* Send the model to the GPU\n",
    "* In the training loop, move your labels and images to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ca0888a-e153-4c29-9759-27fab123964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Convolutional_model().to(device)\n",
    "optimizer = optim.SGD(conv_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f19ff836-9fe6-4b37-bd09-8e884e035d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, loss_fn, model, train_dataloader, test_dataloader,device):\n",
    "    for epoch in range (n_epochs):\n",
    "        \n",
    "        loss_sum = 0.0\n",
    "        for batch_no, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            data, labels = data\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            yhat = model(data) # make predictions\n",
    "            loss = loss_fn(yhat,labels) # calculate the loss\n",
    "            \n",
    "            optimizer.zero_grad() # zero the gradients of our model's parameters\n",
    "            loss.backward() # calculate gradients of the model's parameters\n",
    "            optimizer.step() # will change the model parameters to reduce the loss\n",
    "        \n",
    "            loss_sum = loss_sum + loss.item()\n",
    "            \n",
    "            if batch_no % 2000 == 1999:\n",
    "                print(\"Epoch: {}, batch: {} Loss: {}\".format(epoch,batch_no+1, loss_sum/2000))\n",
    "                loss_sum=0.0\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a91cb04f-8c4e-42fd-be9f-7915d09ec673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch: 2000 Loss: 2.1106125883460045\n",
      "Epoch: 0, batch: 4000 Loss: 1.7368128111660481\n",
      "Epoch: 0, batch: 6000 Loss: 1.5954589762985707\n",
      "Epoch: 0, batch: 8000 Loss: 1.5135901090055703\n",
      "Epoch: 0, batch: 10000 Loss: 1.4722749764099716\n",
      "Epoch: 0, batch: 12000 Loss: 1.4169055113941431\n",
      "Epoch: 1, batch: 2000 Loss: 1.346021385639906\n",
      "Epoch: 1, batch: 4000 Loss: 1.3340802386328579\n",
      "Epoch: 1, batch: 6000 Loss: 1.3128398756757378\n",
      "Epoch: 1, batch: 8000 Loss: 1.2929458208121358\n",
      "Epoch: 1, batch: 10000 Loss: 1.274698679715395\n",
      "Epoch: 1, batch: 12000 Loss: 1.2658963941503316\n",
      "Epoch: 2, batch: 2000 Loss: 1.1792703216560185\n",
      "Epoch: 2, batch: 4000 Loss: 1.16826039968431\n",
      "Epoch: 2, batch: 6000 Loss: 1.166512480167672\n",
      "Epoch: 2, batch: 8000 Loss: 1.2012655458953232\n",
      "Epoch: 2, batch: 10000 Loss: 1.1382859383532777\n",
      "Epoch: 2, batch: 12000 Loss: 1.1751224763672798\n",
      "CPU times: user 1min 36s, sys: 9.35 s, total: 1min 46s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_loop(n_epochs=3,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=loss_fn,\n",
    "              model=conv_model,\n",
    "             train_dataloader=train_dataloader,\n",
    "             test_dataloader=test_dataloader,\n",
    "             device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860dbc76-315e-4ad8-8eb1-f7072de7818c",
   "metadata": {},
   "source": [
    "If you load the train parameters from file, indicates on which device the parameters should be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4249eca-3113-460f-bdac-832e03dc4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../models/cifar10_conv_model.pth'\n",
    "loaded_state_dict = torch.load(file_name,map_location=device) # <- indicates whether the data should go to the GPU or CPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bc2237c4-256c-4cbd-8824-cd008b3ef125",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Convolutional_model()\n",
    "conv_model.load_state_dict(loaded_state_dict)\n",
    "conv_model = conv_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01aa28ae-95fb-44ee-8d83-057d3fcb7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_accuracy(model, dataloader,device):\n",
    "    \"\"\"\n",
    "    Function to calculate the classification accuracy of a model on a given dataset.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return correct*100/total, correct,total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7504eb95-0c8f-41ce-8efc-be56e7a4adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy,_,_ = evaluate_model_accuracy(conv_model,train_dataloader,device)\n",
    "test_accuracy,_,_  = evaluate_model_accuracy(conv_model,test_dataloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "105baf0b-e809-4428-9789-52251154d78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 61.626\n",
      "Accuracy on test set: 57.59\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training set:\", train_accuracy)\n",
    "print(\"Accuracy on test set:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5187eb4-a8b3-4308-9502-d2f13f0ff6c5",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0554c-dfbd-4b52-9c1b-edcc748f5dc2",
   "metadata": {},
   "source": [
    "Try modifying your convolutional neural network to improve classification accuracy on the test dataset. For example, by adding more convolutional filters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae8e6c-ee02-43d0-89e1-c5c4332f1fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2356328b-efda-45f0-8833-ab8ee79aefbf",
   "metadata": {},
   "source": [
    "Train a convolutional neural network to classifiy images of the MNIST dataset. In MNIST, the images have only one color channel instead of 3. They have a shape of (1,28,28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70971599-f097-4965-85f4-6969301fc050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
