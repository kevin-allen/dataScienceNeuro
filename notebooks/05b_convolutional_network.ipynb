{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9161096d-a769-4995-acc2-1b7611dcd5b3",
   "metadata": {},
   "source": [
    "# Week 5, Data science in Neuroscience\n",
    "\n",
    "\n",
    "## Plan for this week\n",
    "\n",
    "1. Introduction to pytorch\n",
    "2. Tensors\n",
    "3. Training loop in pytorch\n",
    "4. A first neural network\n",
    "5. Convolutional networks to process images\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dcbbc1-2c6a-409d-9f0b-47f8f736197a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convolutional neural networks (CNN)\n",
    "\n",
    "Date back to 1989 (Yann LeCun). \n",
    "\n",
    "Convolutional neural network revolutionized how images are processed by neural network. \n",
    "\n",
    "They are used for all sorts of problems involving image processing.\n",
    "\n",
    "The artificial neurons in the network have receptive fields, similar to what is observed in the visual system.\n",
    "\n",
    "Why they work:\n",
    "\n",
    "* They keep information about the spatial arrangement of the input image.\n",
    "* Mechanism to detect a set a feature, independently of their position in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a26923-0db3-4506-83c4-227b23354c32",
   "metadata": {},
   "source": [
    "## Convolution \n",
    "\n",
    "<div>\n",
    "<img src=\"../images/convolution1.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "## LeNet: one of the first convolutional neural network.\n",
    "\n",
    "LeCun et al. (1989). Backpropagation applied to handwritten zip code recognition. Neural Computation.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/LeNet5.png\" width=\"1200\"/>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f889a-1a19-47ce-a92c-230ba0ecbe84",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks as feature detectors\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/imageNet_features_01.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/imageNet_features_02.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/imageNet_features_03.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/imageNet_features_04.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27170ff-f047-4931-97d1-d7cc4b970941",
   "metadata": {},
   "source": [
    "## ImageNet competition: 1.3 million images and 1000 classes\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/imageNetResults.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c733d6-b5dd-4a4c-8c0c-23f7df6b08bc",
   "metadata": {},
   "source": [
    "# Let's build and train a convolutional neural network\n",
    "\n",
    "Our aim will be to build a convolutional neural network to identify what object is in images. This is a classification problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4adfdb6-d811-4420-a98b-5819224e9d33",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We need a dataset of imagest to train our model.\n",
    "\n",
    "We download our dataset using torchvision.datasets. We will use the CIFAR-10 dataset. It consists of 60000 small 32x32 color images.\n",
    "\n",
    "https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0d65d4-ddfe-41d8-8503-d5cc776a401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5446d36b-b326-4b90-a544-007c7cd80fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817ac67e-9779-49f8-91c1-a050699344bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ../data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ae3378-3e8d-404a-8ae1-7316815c2c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ../data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1ecb15-3780-4c85-8fb3-f2cbd9cb7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label = train_dataset[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9271ac9-661d-4d33-a56d-f20dec98cb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c39408-3d5e-416d-8160-c94021aacd87",
   "metadata": {},
   "source": [
    "The shame is [Color, Height, Width]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8cf66e-ac35-4635-b549-d3e600a26218",
   "metadata": {},
   "source": [
    "The first dimension of size 3 represent the color channels. In this case, we only have one color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1476b4-cf7a-4043-a910-8459e0caaf79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e247f-b549-49ae-bb82-b583b20af699",
   "metadata": {},
   "source": [
    "The labels are just numbers. We can associate a word to these numbers by creating a list of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869120bf-7e4e-44e9-abf5-ac3de949e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1e4317-7b86-4edf-b10a-66d8f98f34b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011804a-5deb-4678-aa6d-23deafcc1607",
   "metadata": {},
   "source": [
    "Plot a few images and labels from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8dd1b70-5b7e-4741-9d9c-a0732368b0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b224e7-27e8-47f0-8e7c-d085779c2923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.permute(1,2,0)) # we use permute because imshow() wants [Height,Width,Color] and the tensor is [Color,Height,Width].\n",
    "plt.title(\"This is a {}\".format(class_names[label]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd60794c-ec36-4a68-ab97-f9525fbbcc25",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a figure with several subplots. In each subplot, show the image with imshow() and put the label as title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1602a3-bec8-44d9-b0c9-553d830d03e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4304e70-7e12-4eb1-b9b2-6e3d6cbd8e6c",
   "metadata": {},
   "source": [
    "## Transforms and normalization\n",
    "\n",
    "To facilitate learning:\n",
    "\n",
    "* Normalize our input data so that they have a mean of 0 and a standard deviation of 1\n",
    "* Make sure that all features or color channels have the same distribution. \n",
    "\n",
    "Making sure that the different features have the same distribution means that the choosen learning rate will be appropriate for all features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85665593-8f4e-43f8-95af-ba7dcee179e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = torch.stack([img for img,_ in train_dataset],dim=3)\n",
    "#a.shape\n",
    "#a.view(3,-1).mean(1),a.view(3,-1).std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8867fff-da07-4bf2-b750-e232dba81548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                           torchvision.transforms.Normalize((0.4915,0.4823,0.4468), # means of each color channel\n",
    "                                                                            (0.2470,0.2435,0.2616))]) # standard deviation of each color channel\n",
    "\n",
    "# We pass the transform function to the dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cec511e-0bba-4e1e-a4bb-9f11a40c98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = torch.stack([img for img,_ in train_dataset],dim=3)\n",
    "#a.shape\n",
    "#a.view(3,-1).mean(1),a.view(3,-1).std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f7eda2e-289e-4d75-a5b1-e20895f20c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label = train_dataset[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce1fcc8b-ea16-49ca-947a-135e85637fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmzUlEQVR4nO3de3hU9b3v8c9wyQCSTAzk2oS7ck/cotBsKipJCekRodJdvLQ7KKJAsALa3dJTBWx9onhqizwR3dbC1iNgZYuIPYLhknhpwhaEjVTNAYyChYSCzQwEM1zmd/7wODWGkFnJDL9MeL+eZz0Ps9Z3fuu7XIGPa9bKb1zGGCMAAC6wDrYbAABcnAggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggtFulpaVyuVxas2ZNs7VTp05Vnz59wrJfl8ulhQsXhmUsoD3rZLsBwAmXyxVS3datWyPcCYDWIoAQVZ5//vkGr5977jmVlJQ0Wj948GB9+OGHIY/7zDPPKBAIhKXHL774Qp068VcLaA5/SxBVfvSjHzV4XVFRoZKSkkbrJTkKoM6dO7e6t6906dIlbGO1VcYY1dfXq2vXrrZbQRTjHhDavUAgoIcffljp6enq0qWLcnJytG/fvgY157oHtHr1ao0YMUKxsbGKi4vT8OHDtWTJkmb39817QMePH9ecOXPUp08fud1uJSUl6bvf/a7ee++9847z6aefatasWRo4cKC6du2qHj166F/+5V/0ySefhHzcS5Ys0fDhw9WlSxclJiZq/Pjx2r59e7Bm+fLlGjt2rJKSkuR2uzVkyBAtW7as0Vh9+vTRDTfcoI0bN+qqq65S165d9fTTT4fUB9AUroDQ7j3yyCPq0KGD7r//fnm9Xi1evFi33Xabtm3b1uR7SkpKdMsttygnJ0ePPvqopC+vqN555x3de++9jvY/Y8YMrVmzRrNnz9aQIUN07Ngxvf322/rwww915ZVXNvm+d999V3/+85918803Kz09XZ988omWLVum6667Th988IG6det23v1OmzZNK1asUH5+vu68806dOXNGb731lioqKnTVVVdJkpYtW6ahQ4fqxhtvVKdOnbR+/XrNmjVLgUBAhYWFDcarrKzULbfcorvvvlvTp0/XwIEDHf13ABoxQBQrLCw0Tf0Yb9261UgygwcPNn6/P7h+yZIlRpJ5//33g+sKCgpM7969g6/vvfdeExcXZ86cOeO4J0lmwYIFwdcej8cUFhY6HufkyZON1pWXlxtJ5rnnnjvve7ds2WIkmZ/85CeNtgUCgfPuIy8vz/Tr16/But69extJZsOGDaG2DzSLj+DQ7t1+++2KiYkJvr7mmmskSR9//HGT74mPj1ddXZ1KSkpavf/4+Hht27ZNhw4dcvS+r99fOX36tI4dO6YBAwYoPj6+2Y/v/vM//1Mul0sLFixotO3rTxJ+fR9er1dHjx7Vtddeq48//lher7fB+/r27au8vDxHxwCcDwGEdq9Xr14NXl966aWSpL///e9NvmfWrFm6/PLLlZ+fr/T0dN1xxx3asGFDi/a/ePFi7dmzRxkZGRo5cqQWLlx43vD7yhdffKEHH3xQGRkZcrvd6tmzpxITE1VbW9soHL5p//79SktLU0JCwnnr3nnnHeXm5uqSSy5RfHy8EhMT9Ytf/EKSzhlAQDgRQGj3OnbseM715jzfRp+UlKRdu3bp1Vdf1Y033qitW7cqPz9fBQUFjvf/wx/+UB9//LGWLl2qtLQ0PfbYYxo6dKhef/31877vnnvu0cMPP6wf/vCH+uMf/6g33nhDJSUl6tGjR1geGd+/f79ycnJ09OhRPf744/rTn/6kkpISzZ07V5Ia7YMn3hBuPIQANCEmJkYTJkzQhAkTFAgENGvWLD399NN64IEHNGDAAEdjpaamatasWZo1a5aOHDmiK6+8Ug8//LDy8/ObfM+aNWtUUFCg3/zmN8F19fX1qq2tbXZ//fv318aNG/X55583eRW0fv16+f1+vfrqqw2uEvklXlwoXAEB53Ds2LEGrzt06KDMzExJkt/vD3mcs2fPNvooKykpSWlpac2O07Fjx0ZXaUuXLtXZs2eb3e/kyZNljNGiRYsabftqzK+uDL++D6/Xq+XLlzc7PhAOXAEB53DnnXfq888/19ixY5Wenq5PP/1US5cu1RVXXKHBgweHPM7x48eVnp6uH/zgB8rKylL37t21adMmvfvuuw2ubM7lhhtu0PPPPy+Px6MhQ4aovLxcmzZtUo8ePZrd7/XXX68f//jHeuKJJ7R3716NHz9egUBAb731lq6//nrNnj1b48aNC17l3X333Tpx4oSeeeYZJSUl6fDhwyEfI9BSBBBwDj/60Y/07//+73ryySdVW1urlJQUTZkyRQsXLlSHDqF/cNCtWzfNmjVLb7zxhl5++WUFAgENGDBATz75pGbOnHne9y5ZskQdO3bUCy+8oPr6eo0ePVqbNm0K+Um05cuXKzMzU88++6x++tOfyuPx6KqrrtI///M/S5IGDhyoNWvW6Je//KXuv/9+paSkaObMmUpMTNQdd9wR8jECLeUy57sTCwBAhHAPCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAK9rc7wEFAgEdOnRIsbGxDWbtBQBEB2OMjh8/rrS0tPP+3lybC6BDhw4pIyPDdhsAgFY6ePCg0tPTm9ze5gIoNjbWdgtAVPvxkFRH9c9/0P6n3XnutzHNF33Nu3895ah+6f8KvXbYBEdD6+bvhV6bdrmzscdkhV47aVzotWfPSh/+d/P/nkcsgIqLi/XYY4+purpaWVlZWrp0qUaOHNns+/jYDWidmI7c2v2mbl2d/bvidkeoEUkdOzur7+LgWzC6dXc2dmxc6LVNfKvJeTX373lEflJffPFFzZs3TwsWLNB7772nrKws5eXl6ciRI5HYHQAgCkUkgB5//HFNnz5dt99+u4YMGaKnnnpK3bp10x/+8IdGtX6/Xz6fr8ECAGj/wh5Ap06d0o4dO5Sbm/uPnXTooNzcXJWXlzeqLyoqksfjCS48gAAAF4ewB9DRo0d19uxZJScnN1ifnJys6urqRvXz58+X1+sNLgcPHgx3SwCANsj6U3But1vuSN7xAwC0SWG/AurZs6c6duyompqaButramqUkpIS7t0BAKJU2AMoJiZGI0aM0ObNm4PrAoGANm/erOzs7HDvDgAQpSLyEdy8efNUUFCgq666SiNHjtTvfvc71dXV6fbbb4/E7gAAUSgiATRlyhT97W9/04MPPqjq6mpdccUV2rBhQ6MHEwCE37Pv/9V2CxeEk7kNLktf7GjsyXdd6ah+65vXhFybf6OjoXWVgw+OPnL4DNfOD0Ov7TM49NrTp6Q97zVfF7GHEGbPnq3Zs2dHangAQJRjzg4AgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUuY4yx3cTX+Xw+eTwe220AQMjuvjX02hPxzsbu4qA2NtXZ2MfPhF77bLGDgQOSPpe8Xq/i4uKaLOMKCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWNHJdgMAEO12vR96bZ/BzsauqAq9tmqvs7FPOimudTZ2KLgCAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKxwGWOM7Sa+zufzyePx2G4DANBKXq9XcXFxTW7nCggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWhD2AFi5cKJfL1WAZNGhQuHcDAIhynSIx6NChQ7Vp06Z/7KRTRHYDAIhiEUmGTp06KSUlJRJDAwDaiYjcA9q7d6/S0tLUr18/3XbbbTpw4ECTtX6/Xz6fr8ECAGj/wh5Ao0aN0ooVK7RhwwYtW7ZMVVVVuuaaa3T8+PFz1hcVFcnj8QSXjIyMcLcEAGiDIv6V3LW1terdu7cef/xxTZs2rdF2v98vv98ffO3z+QghAGgHmvtK7og/HRAfH6/LL79c+/btO+d2t9stt9sd6TYAAG1MxH8P6MSJE9q/f79SU1MjvSsAQBQJewDdf//9Kisr0yeffKI///nP+v73v6+OHTvqlltuCfeuAABRLOwfwX322We65ZZbdOzYMSUmJuo73/mOKioqlJiYGO5dAQCiWMQfQnDK5/PJ4/HYbgMA0ErNPYTAXHAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArHAcQG+++aYmTJigtLQ0uVwuvfLKKw22G2P04IMPKjU1VV27dlVubq727t0brn4BAO2E4wCqq6tTVlaWiouLz7l98eLFeuKJJ/TUU09p27ZtuuSSS5SXl6f6+vpWNwsAaEdMK0gya9euDb4OBAImJSXFPPbYY8F1tbW1xu12m1WrVoU0ptfrNZJYWFhYWKJ88Xq95/33Pqz3gKqqqlRdXa3c3NzgOo/Ho1GjRqm8vPyc7/H7/fL5fA0WAED7F9YAqq6uliQlJyc3WJ+cnBzc9k1FRUXyeDzBJSMjI5wtAQDaKOtPwc2fP19erze4HDx40HZLAIALIKwBlJKSIkmqqalpsL6mpia47Zvcbrfi4uIaLACA9i+sAdS3b1+lpKRo8+bNwXU+n0/btm1TdnZ2OHcFAIhynZy+4cSJE9q3b1/wdVVVlXbt2qWEhAT16tVLc+bM0a9//Wtddtll6tu3rx544AGlpaVp0qRJ4ewbABDtnD56vXXr1nM+bldQUBB8FPuBBx4wycnJxu12m5ycHFNZWRny+DyGzcLCwtI+luYew3YZY4zaEJ/PJ4/HY7sNAEAreb3e897Xt/4UHADg4kQAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVnWw3ALRVEx3UrotYF0D7xRUQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwgrngcNH4tcP6//nOvSHX9hi9xNHYnzvsBWiPuAICAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArHAZY4ztJr7O5/PJ4/HYbgPQGge1k//J2dgv7nRWPyW/R8i1rtePORsciBCv16u4uLgmt3MFBACwggACAFjhOIDefPNNTZgwQWlpaXK5XHrllVcabJ86dapcLleDZfz48eHqFwDQTjgOoLq6OmVlZam4uLjJmvHjx+vw4cPBZdWqVa1qEgDQ/jj+PqD8/Hzl5+eft8btdislJaXFTQEA2r+I3AMqLS1VUlKSBg4cqJkzZ+rYsaafyvH7/fL5fA0WAED7F/YAGj9+vJ577jlt3rxZjz76qMrKypSfn6+zZ8+es76oqEgejye4ZGRkhLslAEAbFPav5L755puDfx4+fLgyMzPVv39/lZaWKicnp1H9/PnzNW/evOBrn89HCAHARSDij2H369dPPXv21L59+8653e12Ky4ursECAGj/Ih5An332mY4dO6bU1NRI7woAEEUcfwR34sSJBlczVVVV2rVrlxISEpSQkKBFixZp8uTJSklJ0f79+/Vv//ZvGjBggPLy8sLaOAAgujmeC660tFTXX399o/UFBQVatmyZJk2apJ07d6q2tlZpaWkaN26cfvWrXyk5OTmk8ZkLDpGy+rXdjur3/OHpkGsXvdz078Wdy32OqqXfOKhd39PZ2JOOOqt3YuLwb4Vcu+79v0auEVjR3Fxwjq+ArrvuOp0vszZu3Oh0SADARYi54AAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArHM8FF2nMBYdIieiP+n+UOip3TW08n+L5xDio9f9+mqOxH7jz2ZBrf+1oZOnT3z8Ucu1PXljtaOx1Wz9w2E3kJDmovdTh2JUO69uS5uaC4woIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsKKT7QZgn9MJavo4rP/UYX2kuFwuR/Xm0InQi99Y42jsgY6qnU3H8icHU+tI0mEHtbc5Glnqe+eDIdcGHI6dlh567R9qnY2dN9jZVEmSg5+Vy/o7G7rqWOi15SXOxo4Qn6RQJlTjCggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFjBXHBRwul8bZH0fxzWD41IF5FXPGVkyLWd3vrA0divOpxq7PI/OZlRr7ujsf+H/hpyratbpqOxnUhwMLebJN11Jjnk2rzBoddKkv73Q87qL/+Os/pIGXeLs/qS1ZHpI0RcAQEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWuIwxbWmWF/l8Pnk8HtttXBBt6j98BM1wUPt0xLqIrCSH9TWO9+Bk1qwzjkcHwsknySPJ6/UqLi6uyTqugAAAVhBAAAArHAVQUVGRrr76asXGxiopKUmTJk1SZWVlg5r6+noVFhaqR48e6t69uyZPnqyaGucfOAAA2jdHAVRWVqbCwkJVVFSopKREp0+f1rhx41RXVxesmTt3rtavX6+XXnpJZWVlOnTokG666aawNw4AiG6tegjhb3/7m5KSklRWVqYxY8bI6/UqMTFRK1eu1A9+8ANJ0kcffaTBgwervLxc3/72txuN4ff75ff7g699Pp8yMjJa2lJU4SGExngIoSk8hIDocUEeQvB6vZKkhIQESdKOHTt0+vRp5ebmBmsGDRqkXr16qby8/JxjFBUVyePxBJeLJXwA4GLX4gAKBAKaM2eORo8erWHDhkmSqqurFRMTo/j4+Aa1ycnJqq6uPuc48+fPl9frDS4HDx5saUsAgCjS4q/kLiws1J49e/T222+3qgG32y23292qMQAA0adFV0CzZ8/Wa6+9pq1btyo9/R9f5J6SkqJTp06ptra2QX1NTY1SUlJa1SgAoH1xFEDGGM2ePVtr167Vli1b1Ldv3wbbR4wYoc6dO2vz5s3BdZWVlTpw4ICys7PD0zEAoF1w9BFcYWGhVq5cqXXr1ik2NjZ4X8fj8ahr167yeDyaNm2a5s2bp4SEBMXFxemee+5Rdnb2OZ+AAwBcvBw9hu1yuc65fvny5Zo6daqkL38R9b777tOqVavk9/uVl5enJ598MuSP4NraXHCjHdS27m4YEAVSrnFWP/hKB7W9nI19aXLotX93+OB7V4e3x/NvcDB2d2dj93TwkL/Tu/r9uzgo9jdf8v+F+hi2o3ZDyaouXbqouLhYxcXFToYGAFxkmAsOAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGBFq74RNRK+moonRaGnY6KD8U847Gefw/qLg8P5PgbfHXrt9dc7GzvDwXQsVX91NvbLq0OvPfqKs7EdczJNjdPvWw19ihWEg8OpxlIcTOR83wRnY+918C/i3v8bcqnvzCl53no+st+ICgBASxFAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBVtdi64H0uKCfE9GQ7GH+SwnykO6y8Kna52Vn/m3cj0AaBN8unLGe+YCw4A0CYRQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKzrZbqApxyR1DrG22sG4C1rQC76BqXWANukKh/X/HYkmHOAKCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWNFm54IbI6lLiLW1EewjWp10ULvH4dhOf2iudFgP4B9udlBre243p7gCAgBY4SiAioqKdPXVVys2NlZJSUmaNGmSKisrG9Rcd911crlcDZYZM2aEtWkAQPRzFEBlZWUqLCxURUWFSkpKdPr0aY0bN051dXUN6qZPn67Dhw8Hl8WLF4e1aQBA9HP0cf6GDRsavF6xYoWSkpK0Y8cOjRkzJri+W7duSklJCU+HAIB2qVX3gLxeryQpISGhwfoXXnhBPXv21LBhwzR//nydPNn0LXG/3y+fz9dgAQC0fy1+Ci4QCGjOnDkaPXq0hg0bFlx/6623qnfv3kpLS9Pu3bv1s5/9TJWVlXr55ZfPOU5RUZEWLVrU0jYAAFHKZYwxLXnjzJkz9frrr+vtt99Wenp6k3VbtmxRTk6O9u3bp/79+zfa7vf75ff7g699Pp8yMjL0iCLzGPbF8pXcPIYNtA9OHsN+MWJdtIzX61VcXFyT21t0BTR79my99tprevPNN88bPpI0atQoSWoygNxut9xud0vaAABEMUcBZIzRPffco7Vr16q0tFR9+/Zt9j27du2SJKWmpraoQQBA++QogAoLC7Vy5UqtW7dOsbGxqq6uliR5PB517dpV+/fv18qVK/W9731PPXr00O7duzV37lyNGTNGmZmZETkAAEB0chRAy5Ytk/TlL5t+3fLlyzV16lTFxMRo06ZN+t3vfqe6ujplZGRo8uTJ+uUvfxm2hgEA7UOLH0KIFJ/PJ4/Ho5VJUrcQHxKPrw59/Gtb1pZ1LtsNtFFt6ocXiIBI/t2/7Z8mhVw7fPi3Qq6tP3VKC1c/0+xDCMwFBwCwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFjR4i+ki7SaI1LXEGsnRrSTyPkP2w20kNP/awlEpIsv7XZQy3S4aAsO2G7ga17Y+UrItd12hj5uqFNkcQUEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsaLNzwV2SJHULMR6XVIc+7r0tayciptpuoIUiObebU1kOakOdnwqIpGW2G2ihkxEYkysgAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwIo2OxVPcop0ScfQap93MBXPQw77+NxhfTSa7LDe6Q/Niw7rgfbssO0GvuZaB7X1DmrPSNoRQh1XQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwIo2Oxfc0CHdFBvjCqk2fWddyONubGlD7dj0pasd1e95db2j+hdLXnBUHykeh/W+iHQBtB0OptFUny6h154xkvzN13EFBACwwlEALVu2TJmZmYqLi1NcXJyys7P1+uuvB7fX19ersLBQPXr0UPfu3TV58mTV1NSEvWkAQPRzFEDp6el65JFHtGPHDm3fvl1jx47VxIkT9Ze//EWSNHfuXK1fv14vvfSSysrKdOjQId10000RaRwAEN0c3QOaMGFCg9cPP/ywli1bpoqKCqWnp+vZZ5/VypUrNXbsWEnS8uXLNXjwYFVUVOjb3/52+LoGAES9Ft8DOnv2rFavXq26ujplZ2drx44dOn36tHJzc4M1gwYNUq9evVReXt7kOH6/Xz6fr8ECAGj/HAfQ+++/r+7du8vtdmvGjBlau3athgwZourqasXExCg+Pr5BfXJysqqrm37WoqioSB6PJ7hkZGQ4PggAQPRxHEADBw7Url27tG3bNs2cOVMFBQX64IMPWtzA/Pnz5fV6g8vBgwdbPBYAIHo4/j2gmJgYDRgwQJI0YsQIvfvuu1qyZImmTJmiU6dOqba2tsFVUE1NjVJSUpocz+12y+12O+8cABDVWv17QIFAQH6/XyNGjFDnzp21efPm4LbKykodOHBA2dnZrd0NAKCdcXQFNH/+fOXn56tXr146fvy4Vq5cqdLSUm3cuFEej0fTpk3TvHnzlJCQoLi4ON1zzz3Kzs7mCTgAQCOOAujIkSP613/9Vx0+fFgej0eZmZnauHGjvvvd70qSfvvb36pDhw6aPHmy/H6/8vLy9OSTT7aoscR7bldc99A+mnso8ZWQx93zm48d9bHNUXV0WvCos6l4rhieGaFOIovnK4GG/uag9tEFr4Rce7L+pDYvurXZOkcB9Oyzz553e5cuXVRcXKzi4mInwwIALkLMBQcAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsMLxbNiRZoyRJPnqToX8nuP+QMi1Zxx31P6dCZx2VH/qtD9CnQC4kIyD2pP1J0Ov9X9Z+9W/501xmeYqLrDPPvuML6UDgHbg4MGDSk9Pb3J7mwugQCCgQ4cOKTY2Vi6XK7je5/MpIyNDBw8eVFxcnMUOI4vjbD8uhmOUOM72JhzHaYzR8ePHlZaWpg4dmr7T0+Y+guvQocN5EzMuLq5dn/yvcJztx8VwjBLH2d609jg9Hk+zNTyEAACwggACAFgRNQHkdru1YMECud2hfUldtOI424+L4RgljrO9uZDH2eYeQgAAXByi5goIANC+EEAAACsIIACAFQQQAMAKAggAYEXUBFBxcbH69OmjLl26aNSoUfqv//ov2y2F1cKFC+VyuRosgwYNst1Wq7z55puaMGGC0tLS5HK59MorrzTYbozRgw8+qNTUVHXt2lW5ubnau3evnWZbobnjnDp1aqNzO378eDvNtlBRUZGuvvpqxcbGKikpSZMmTVJlZWWDmvr6ehUWFqpHjx7q3r27Jk+erJqaGksdt0wox3ndddc1Op8zZsyw1HHLLFu2TJmZmcHZDrKzs/X6668Ht1+ocxkVAfTiiy9q3rx5WrBggd577z1lZWUpLy9PR44csd1aWA0dOlSHDx8OLm+//bbtllqlrq5OWVlZKi4uPuf2xYsX64knntBTTz2lbdu26ZJLLlFeXp7q6+svcKet09xxStL48eMbnNtVq1ZdwA5br6ysTIWFhaqoqFBJSYlOnz6tcePGqa6uLlgzd+5crV+/Xi+99JLKysp06NAh3XTTTRa7di6U45Sk6dOnNzifixcvttRxy6Snp+uRRx7Rjh07tH37do0dO1YTJ07UX/7yF0kX8FyaKDBy5EhTWFgYfH327FmTlpZmioqKLHYVXgsWLDBZWVm224gYSWbt2rXB14FAwKSkpJjHHnssuK62tta43W6zatUqCx2GxzeP0xhjCgoKzMSJE630EylHjhwxkkxZWZkx5stz17lzZ/PSSy8Faz788EMjyZSXl9tqs9W+eZzGGHPttdeae++9115TEXLppZea3//+9xf0XLb5K6BTp05px44dys3NDa7r0KGDcnNzVV5ebrGz8Nu7d6/S0tLUr18/3XbbbTpw4IDtliKmqqpK1dXVDc6rx+PRqFGj2t15laTS0lIlJSVp4MCBmjlzpo4dO2a7pVbxer2SpISEBEnSjh07dPr06Qbnc9CgQerVq1dUn89vHudXXnjhBfXs2VPDhg3T/PnzdfJk6N+V09acPXtWq1evVl1dnbKzsy/ouWxzs2F/09GjR3X27FklJyc3WJ+cnKyPPvrIUlfhN2rUKK1YsUIDBw7U4cOHtWjRIl1zzTXas2ePYmNjbbcXdtXV1ZJ0zvP61bb2Yvz48brpppvUt29f7d+/X7/4xS+Un5+v8vJydezY0XZ7jgUCAc2ZM0ejR4/WsGHDJH15PmNiYhQfH9+gNprP57mOU5JuvfVW9e7dW2lpadq9e7d+9rOfqbKyUi+//LLFbp17//33lZ2drfr6enXv3l1r167VkCFDtGvXrgt2Ltt8AF0s8vPzg3/OzMzUqFGj1Lt3b/3xj3/UtGnTLHaG1rr55puDfx4+fLgyMzPVv39/lZaWKicnx2JnLVNYWKg9e/ZE/T3K5jR1nHfddVfwz8OHD1dqaqpycnK0f/9+9e/f/0K32WIDBw7Url275PV6tWbNGhUUFKisrOyC9tDmP4Lr2bOnOnbs2OgJjJqaGqWkpFjqKvLi4+N1+eWXa9++fbZbiYivzt3Fdl4lqV+/furZs2dUntvZs2frtdde09atWxt8b1dKSopOnTql2traBvXRej6bOs5zGTVqlCRF3fmMiYnRgAEDNGLECBUVFSkrK0tLliy5oOeyzQdQTEyMRowYoc2bNwfXBQIBbd68WdnZ2RY7i6wTJ05o//79Sk1Ntd1KRPTt21cpKSkNzqvP59O2bdva9XmVvvza+WPHjkXVuTXGaPbs2Vq7dq22bNmivn37Ntg+YsQIde7cucH5rKys1IEDB6LqfDZ3nOeya9cuSYqq83kugUBAfr//wp7LsD7SECGrV682brfbrFixwnzwwQfmrrvuMvHx8aa6utp2a2Fz3333mdLSUlNVVWXeeecdk5uba3r27GmOHDliu7UWO378uNm5c6fZuXOnkWQef/xxs3PnTvPpp58aY4x55JFHTHx8vFm3bp3ZvXu3mThxounbt6/54osvLHfuzPmO8/jx4+b+++835eXlpqqqymzatMlceeWV5rLLLjP19fW2Ww/ZzJkzjcfjMaWlpebw4cPB5eTJk8GaGTNmmF69epktW7aY7du3m+zsbJOdnW2xa+eaO859+/aZhx56yGzfvt1UVVWZdevWmX79+pkxY8ZY7tyZn//856asrMxUVVWZ3bt3m5///OfG5XKZN954wxhz4c5lVASQMcYsXbrU9OrVy8TExJiRI0eaiooK2y2F1ZQpU0xqaqqJiYkx3/rWt8yUKVPMvn37bLfVKlu3bjWSGi0FBQXGmC8fxX7ggQdMcnKycbvdJicnx1RWVtptugXOd5wnT54048aNM4mJiaZz586md+/eZvr06VH3P0/nOj5JZvny5cGaL774wsyaNctceumlplu3bub73/++OXz4sL2mW6C54zxw4IAZM2aMSUhIMG632wwYMMD89Kc/NV6v127jDt1xxx2md+/eJiYmxiQmJpqcnJxg+Bhz4c4l3wcEALCizd8DAgC0TwQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYMX/A5QnRpyXhCH2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.permute(1,2,0)) # we use permute because imshow() wants H x W x C and the tensor is C X H X W.\n",
    "plt.title(\"This is a {}\".format(class_names[label]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886d1b4-51f1-449c-985f-38ddcb3c86e1",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "We usually want to feed a few images at a time to our neural network. With large models and datasets, we would not be able to fit all that data in the GPU or computer memory. So we feed a few items at a time. \n",
    "\n",
    "The small group of images that we are feeding to the network simultaneously is called a `batch`.\n",
    "\n",
    "The number of images that we feed to the neural network simultaneously is called the `batch size`.\n",
    "\n",
    "pytorch has a `dataloader` class that can help you getting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2ac7cca-4a95-4a5a-813d-9d38556d1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True, \n",
    "                                           num_workers=2, \n",
    "                                           pin_memory=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef8ff6a0-c7c7-4e24-973a-6cf83b68ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96aedc2c-ecf3-4519-a3e4-219cf6188f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac626ea-b370-48f4-bd28-9ac7342d39d7",
   "metadata": {},
   "source": [
    "The shape is [Batch, Color, Height, Width]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5199294-033c-452f-a8ba-9ad983ad40f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941daf62-af7b-41cd-9846-54a613b4fbd3",
   "metadata": {},
   "source": [
    "## Our model\n",
    "\n",
    "We can build our model using the class from the last class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6f32f-6dfc-48ea-a496-8877033a51c0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../images/LeNet5.png\" width=\"1200\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92a941bc-b409-4be1-9410-f5d81d862820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Convolutional_model(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to create convolutional neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializer. Runs when an object is created\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # input images will be batch_size,3,32,32\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels= 6, kernel_size=5,stride=1, padding=0)  # output shape: [batch_size, 6, 28, 28]\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2) # a max poll operation, # output shape: [batch_size, 6, 14,14]\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5,stride=1, padding=0)  # output shape: [batch_size, 16, 10, 10]\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)                                                   # output shape: [batch_size, 16 , 5, 5]\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 16 * 5 * 5, out_features=120) # here we need to know the dimension of the data coming in (following second pool operation)\n",
    "        self.fc2 = nn.Linear(in_features = 120, out_features =84)\n",
    "        self.fc3 = nn.Linear(in_features = 84, out_features=10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Make predictions with our model\n",
    "        \"\"\"\n",
    "\n",
    "        # We have 2 x pool(relu(conv()))\n",
    "        # This is the part extracting visual features\n",
    "        \n",
    "        # input shape [batch,3,32,32]\n",
    "        x = F.relu(self.conv1(x)) # [batch, 6, 28, 28]\n",
    "        x = self.pool1(x) # [batch, 6, 14, 14]\n",
    "        \n",
    "        x = F.relu(self.conv2(x)) # [batch, 16, 10, 10]\n",
    "        x = self.pool2(x) # [batch, 16, 5, 5]\n",
    "\n",
    "        # We then have 3 linear layers. These layers are take visual features as inputs and find a way to classify the image.\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch, needed for the linear layer, [batch, 400]\n",
    "        x = F.relu(self.fc1(x)) # [batch, 120]\n",
    "        x = F.relu(self.fc2(x)) # [batch, 84]\n",
    "        x = self.fc3(x) # [batch, 10]\n",
    "        return x\n",
    "\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f987f82-9893-4b14-bf0a-2c563514aed6",
   "metadata": {},
   "source": [
    "**If you are implementing a model, start by passing the data through it line by line**\n",
    "\n",
    "To follow how each layer in your network modify the dimension of the data flowing through the network, you can run it line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b09a1e8-e27f-4442-aeb5-a626407e9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=3,out_channels=6,kernel_size=5,stride=1,padding=0)\n",
    "pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "conv2 = nn.Conv2d(in_channels=6,out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "pool2 = nn.MaxPool2d(kernel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "812c78ec-c0ab-436d-8f50-3455f03d4b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([4, 3, 32, 32])\n",
      "output of conv1 shape: torch.Size([4, 6, 28, 28])\n",
      "output of pool shape: torch.Size([4, 6, 14, 14])\n",
      "output of conv2 shape: torch.Size([4, 16, 10, 10])\n",
      "output of pool shape: torch.Size([4, 16, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"images.shape:\",images.shape)\n",
    "res = F.relu(conv1(images))\n",
    "print(\"output of conv1 shape:\", res.shape)\n",
    "res = pool1(res)\n",
    "print(\"output of pool shape:\", res.shape)\n",
    "res = F.relu(conv2(res))\n",
    "print(\"output of conv2 shape:\", res.shape)\n",
    "res = pool2(res)\n",
    "print(\"output of pool shape:\", res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7291ef0e-070b-473e-98bf-e61e5058e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Convolutional_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb93852c-08b6-424e-9b26-7e8b5506de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name,p in conv_model.named_parameters():\n",
    "#    print(name,p.shape,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c008e7c-0778-47c7-a14d-b69ea7eee7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 62006\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters:\", sum(p.numel() for p in conv_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815230e-1da6-414a-a06e-ec4a9c5b8a55",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "We need a loss function that works with categorical data. \n",
    "\n",
    "The last layer has an output size of 10, which is the number of categories we have in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1912a980-5b7b-4f2b-9f94-3a14781433de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(conv_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65aa380-1faf-46ad-a100-2c2ce8712c1b",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b2c4ab6-3311-4f9c-8bcc-bc401778cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, loss_fn, model, train_dataloader, test_dataloader):\n",
    "    for epoch in range (n_epochs):\n",
    "        \n",
    "        loss_sum = 0.0\n",
    "        for batch_no, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            data, labels = data\n",
    "        \n",
    "        \n",
    "            yhat = model(data) # make predictions\n",
    "            loss = loss_fn(yhat,labels) # calculate the loss\n",
    "            \n",
    "            optimizer.zero_grad() # zero the gradients of our model's parameters\n",
    "            loss.backward() # calculate gradients of the model's parameters\n",
    "            optimizer.step() # will change the model parameters to reduce the loss\n",
    "        \n",
    "            loss_sum = loss_sum + loss.item()\n",
    "            \n",
    "            if batch_no % 2000 == 1999:\n",
    "                print(\"Epoch: {}, batch: {} Loss: {}\".format(epoch,batch_no, loss_sum/2000))\n",
    "                loss_sum=0.0\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1c5400c-b551-40fa-8ada-06232f9f3c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch: 1999 Loss: 2.1191408501267435\n",
      "Epoch: 0, batch: 3999 Loss: 1.7494019333422184\n",
      "Epoch: 0, batch: 5999 Loss: 1.6518569350540637\n",
      "Epoch: 0, batch: 7999 Loss: 1.5907758810818196\n",
      "Epoch: 0, batch: 9999 Loss: 1.528643264338374\n",
      "Epoch: 0, batch: 11999 Loss: 1.4544115615785123\n",
      "Epoch: 1, batch: 1999 Loss: 1.397918008711189\n",
      "Epoch: 1, batch: 3999 Loss: 1.3760450061410665\n",
      "Epoch: 1, batch: 5999 Loss: 1.3419032205492258\n",
      "Epoch: 1, batch: 7999 Loss: 1.3262357884086668\n",
      "Epoch: 1, batch: 9999 Loss: 1.3166793277245015\n",
      "Epoch: 1, batch: 11999 Loss: 1.2773284554257989\n",
      "Epoch: 2, batch: 1999 Loss: 1.2212821908667684\n",
      "Epoch: 2, batch: 3999 Loss: 1.2115047412328421\n",
      "Epoch: 2, batch: 5999 Loss: 1.226159394480288\n",
      "Epoch: 2, batch: 7999 Loss: 1.2074957261811943\n",
      "Epoch: 2, batch: 9999 Loss: 1.2002870111167432\n",
      "Epoch: 2, batch: 11999 Loss: 1.2033575446996838\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs=3,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=loss_fn,\n",
    "              model=conv_model,\n",
    "             train_dataloader=train_dataloader,\n",
    "             test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e444fdaa-717c-48f2-8fa6-fc8ffc4cd269",
   "metadata": {},
   "source": [
    "## Saving your model to file \n",
    "\n",
    "With larger models trained on large datasets, you will want to save your trained model so that you don't have to train it from scratch all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "485d8d3c-e8d4-494d-b339-dcc772432a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../models/cifar10_conv_model.pth'\n",
    "torch.save(conv_model.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a39a0-6247-4b73-866e-2fc47cbf3edb",
   "metadata": {},
   "source": [
    "## Loading your model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0467cb58-78d6-4c00-a7ae-b33cc57d2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../models/cifar10_conv_model.pth'\n",
    "loaded_state_dict = torch.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9841c547-f88c-40a3-b2e6-502b08417d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = Convolutional_model()\n",
    "conv_model.load_state_dict(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170539d3-2b77-4f6a-bb4e-47af0965110d",
   "metadata": {},
   "source": [
    "## Evaluate our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3c93866-2c70-4d15-87f5-0894de2f0c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81462fc0-8a97-4092-b250-762cf9041310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 0, 9])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(dataiter)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a01ccbb-b0e8-43b8-a6e3-9f6b0e995cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    res = conv_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42a2b128-3f73-49bd-bbbe-2ab863f88733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9361, -2.8161,  1.2308,  2.6349,  0.7240,  2.3404, -0.1588,  0.8672,\n",
       "         -2.5471, -2.4492],\n",
       "        [ 0.0817,  1.2232, -0.7182, -0.3649, -1.2652, -0.6932, -0.3029, -1.4771,\n",
       "          0.8757,  1.9852],\n",
       "        [ 3.9082, -4.3985,  2.7285, -0.1434,  2.1980, -0.4387, -2.4892, -1.4065,\n",
       "          0.3968, -3.3443],\n",
       "        [ 0.4427,  2.8608, -1.7611, -0.8898, -3.2320, -0.8220, -2.7651, -1.4028,\n",
       "          0.8552,  6.1889]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c6ea4-5256-4cdc-a0a9-65cdda2165f1",
   "metadata": {},
   "source": [
    "We can use the `torch.max()` function to get the index with the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "447ccd94-676b-42b3-bc2c-88d02924dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions = torch.max(res,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23901b84-371e-4ba9-b117-7431f987ec47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 9, 0, 9])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c2a8693-04e3-4edc-8e73-78873beb3a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(labels == predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b06dfe5e-10a4-4814-89cb-f526777150a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_accuracy(model, dataloader):\n",
    "    \"\"\"\n",
    "    Function to calculate the classification accuracy of a model on a given dataset.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return correct*100/total, correct,total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1eb7c630-dc13-4d6d-9f56-e352b0148c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy,_,_ = evaluate_model_accuracy(conv_model,train_dataloader)\n",
    "test_accuracy,_,_  = evaluate_model_accuracy(conv_model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c9d8d69-eff9-4f54-9802-acd37c9e7b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 59.006\n",
      "Accuracy on test set: 55.62\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training set:\", train_accuracy)\n",
    "print(\"Accuracy on test set:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d58b5034-6b18-4955-a8c4-ae426e635426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: plane is 65.1 %\n",
      "Accuracy for class: car   is 71.1 %\n",
      "Accuracy for class: bird  is 51.8 %\n",
      "Accuracy for class: cat   is 48.0 %\n",
      "Accuracy for class: deer  is 37.5 %\n",
      "Accuracy for class: dog   is 54.0 %\n",
      "Accuracy for class: frog  is 43.4 %\n",
      "Accuracy for class: horse is 54.6 %\n",
      "Accuracy for class: ship  is 70.4 %\n",
      "Accuracy for class: truck is 60.3 %\n"
     ]
    }
   ],
   "source": [
    "correct_pred = {classname: 0 for classname in class_names}\n",
    "total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = conv_model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[class_names[label]] += 1\n",
    "            total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d96b58d-388c-4f31-a815-ca226401d294",
   "metadata": {},
   "source": [
    "You now know how Google and co. classify the images that you upload to their servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6adeb-e96e-4d30-bd54-bc75efd0098a",
   "metadata": {},
   "source": [
    "## Let's have a look at the filter in our first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b900b6a2-0418-46ee-9b18-4149ec579393",
   "metadata": {},
   "outputs": [],
   "source": [
    "myIter = iter(conv_model.named_parameters())\n",
    "name, param = next(myIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "84e1cc19-56e5-4ca9-90a5-067e1a00a7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('conv1.weight', torch.Size([6, 3, 5, 5]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "829a5a96-5cdc-41fd-a4ca-0449f82afe84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.8374553322792053, 0.8296098709106445)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmin = param.min().detach().numpy().item()\n",
    "pmax = param.max().detach().numpy().item()\n",
    "pmin,pmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bed7f1dc-146c-41af-8881-4107d2a27abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow=1\n",
    "ncol=param.shape[0]\n",
    "fig, ax = plt.subplots(nrow,ncol,figsize=(15,3))\n",
    "for i in range(param.shape[0]):\n",
    "    f = filter0 = param[i].permute(1,2,0).detach().numpy()\n",
    "    f = (f-pmin)/(pmax-pmin)\n",
    "    ax[i].imshow(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988cbe3-79a1-434c-81e4-2061c9c15ad5",
   "metadata": {},
   "source": [
    "## Training networks using a GPU\n",
    "\n",
    "In this notebook, we trained a very small convolutional neural network on a set of very small images. \n",
    "\n",
    "In most applications, you would be working with a larger convolutional network (e.g., [ResNet](https://pytorch.org/vision/main/models/resnet.html)) with images that are often more than 28x28 pixels (e.g., [ImageNet](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageNet.html)).\n",
    "\n",
    "For such task, training would be at least 10x faster using a GPU instead of a CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c918cc6-8fbd-47cf-9f1c-0d3d6f4599a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c54fbd-6d6e-4b63-b40f-bbb0c3b6df1a",
   "metadata": {},
   "source": [
    "You need to make two changes to run your model on a GPU (if you have one)\n",
    "\n",
    "* Send the model to the GPU\n",
    "* In the training loop, move your labels and images to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ca0888a-e153-4c29-9759-27fab123964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Convolutional_model().to(device)\n",
    "optimizer = optim.SGD(conv_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f19ff836-9fe6-4b37-bd09-8e884e035d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, loss_fn, model, train_dataloader, test_dataloader,device):\n",
    "    for epoch in range (n_epochs):\n",
    "        \n",
    "        loss_sum = 0.0\n",
    "        for batch_no, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            data, labels = data\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            yhat = model(data) # make predictions\n",
    "            loss = loss_fn(yhat,labels) # calculate the loss\n",
    "            \n",
    "            optimizer.zero_grad() # zero the gradients of our model's parameters\n",
    "            loss.backward() # calculate gradients of the model's parameters\n",
    "            optimizer.step() # will change the model parameters to reduce the loss\n",
    "        \n",
    "            loss_sum = loss_sum + loss.item()\n",
    "            \n",
    "            if batch_no % 2000 == 1999:\n",
    "                print(\"Epoch: {}, batch: {} Loss: {}\".format(epoch,batch_no, loss_sum/2000))\n",
    "                loss_sum=0.0\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a91cb04f-8c4e-42fd-be9f-7915d09ec673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch: 1999 Loss: 2.0886529904007913\n",
      "Epoch: 0, batch: 3999 Loss: 1.7401047326624393\n",
      "Epoch: 0, batch: 5999 Loss: 1.6010591691881417\n",
      "Epoch: 0, batch: 7999 Loss: 1.5250799492299556\n",
      "Epoch: 0, batch: 9999 Loss: 1.4587319696098566\n",
      "Epoch: 0, batch: 11999 Loss: 1.428734954547137\n",
      "Epoch: 1, batch: 1999 Loss: 1.3491272297017276\n",
      "Epoch: 1, batch: 3999 Loss: 1.3538447453379632\n",
      "Epoch: 1, batch: 5999 Loss: 1.3283158321529627\n",
      "Epoch: 1, batch: 7999 Loss: 1.299119295991957\n",
      "Epoch: 1, batch: 9999 Loss: 1.2669003800302745\n",
      "Epoch: 1, batch: 11999 Loss: 1.2670237898863852\n",
      "Epoch: 2, batch: 1999 Loss: 1.2010140179768205\n",
      "Epoch: 2, batch: 3999 Loss: 1.2035696631371975\n",
      "Epoch: 2, batch: 5999 Loss: 1.1993611356765033\n",
      "Epoch: 2, batch: 7999 Loss: 1.186187446495518\n",
      "Epoch: 2, batch: 9999 Loss: 1.166157171281986\n",
      "Epoch: 2, batch: 11999 Loss: 1.1638242598585784\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs=3,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=loss_fn,\n",
    "              model=conv_model,\n",
    "             train_dataloader=train_dataloader,\n",
    "             test_dataloader=test_dataloader,\n",
    "             device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860dbc76-315e-4ad8-8eb1-f7072de7818c",
   "metadata": {},
   "source": [
    "If you load the train parameters from file, indicates on which device the parameters should be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a4249eca-3113-460f-bdac-832e03dc4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../models/cifar10_conv_model.pth'\n",
    "loaded_state_dict = torch.load(file_name,map_location=device) # <- indicates whether the data should go to the GPU or CPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc2237c4-256c-4cbd-8824-cd008b3ef125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = Convolutional_model()\n",
    "conv_model.load_state_dict(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5187eb4-a8b3-4308-9502-d2f13f0ff6c5",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0554c-dfbd-4b52-9c1b-edcc748f5dc2",
   "metadata": {},
   "source": [
    "Try modifying your convolutional neural network to improve classification accuracy on the test dataset. For example, by adding more convolutional filters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70971599-f097-4965-85f4-6969301fc050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2356328b-efda-45f0-8833-ab8ee79aefbf",
   "metadata": {},
   "source": [
    "Train a convolutional neural network to classifiy images of the MNIST dataset. In MNIST, the images have only one color channel instead of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37458502-a278-4c54-9198-978ea5e2943d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
